Hello guys, welcome back to the channel.

So far,
we have trained deep learning models
from scratch.

But in the real world,
this is rarely done.

Todayâ€™s topic is:
TRANSFER LEARNING WITH RESNET.

Transfer learning means
reusing knowledge from one task
to solve another task.

Instead of training a deep network
from scratch,
we use a pretrained model.

This saves:
time,
computing power,
and data.

Now let us understand ResNet.

ResNet stands for
Residual Network.

It is one of the most famous
deep learning architectures.

ResNet was trained on ImageNet,
which contains millions of images.

This means ResNet already knows
how to detect:
edges,
textures,
shapes,
and objects.

We reuse this knowledge.

Now let us look at the code.

We load the CIFAR-10 dataset.

For simplicity,
we convert it into a binary problem:
airplane versus automobile.

ResNet expects images
of size 224 by 224,
so we resize the images.

Next,
we load the pretrained ResNet50 model.

We remove the top classification layers
and keep only the feature extractor.

We freeze the base model.

This means ResNet weights
will not be updated.

Then we add our own classifier
on top.

This classifier learns
to map ResNet features
to our problem.

We compile and train the model.

Notice something very important.

Training is very fast.

Accuracy is very high,
even with few epochs.

This is the power of transfer learning.

In real-world applications,
transfer learning is used everywhere:
medical imaging,
self-driving cars,
face recognition,
and more.

In the next video,
we will go even deeper
and fine-tune large language models
like BERT.

Thank you for watching.
See you in the next video.
