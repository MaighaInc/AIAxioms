Hello guys, welcome back to the channel.

In the previous videos,
we learned about RNNs and LSTMs.

Today we are learning
a very practical and important model.

Todayâ€™s topic is:
GRU TIME-SERIES PREDICTOR.

GRU stands for
Gated Recurrent Unit.

GRU was introduced
as a simpler alternative to LSTM.

Now let us first understand
why GRU exists.

Simple RNNs have memory problems.
They forget long-term patterns.

LSTMs solved this
by introducing gates.

But LSTMs are complex.
They have many gates
and many parameters.

GRU simplifies this idea.

GRU combines gates
to reduce complexity
while keeping performance strong.

Now let us understand the task.

We are given a sequence of numbers.
The goal is to predict
the next number in the sequence.

This represents time-series data.

Time-series data exists everywhere:
stock prices,
temperature,
sales,
sensor readings.

Now let us look at the code.

First,
we generate numeric sequences.

Each input has four time steps.
The target is the next value.

Next,
we normalize the data.

Normalization is very important
for stable training.

Then we reshape the data.

GRU expects input in the form:
samples,
timesteps,
features.

Now let us build the model.

We use a GRU layer
with 64 units.

This layer processes the sequence
while maintaining memory.

Then we use a Dense layer
to output the prediction.

We compile the model
using mean squared error loss
because this is a regression problem.

We train the model.

During training,
the GRU learns the pattern
of increasing numbers.

Finally,
we test the model
with a new sequence.

The predicted value
is very close to the correct one.

Now let us compare models.

RNN is simple but weak.
LSTM is powerful but heavy.
GRU is balanced.

GRU is often faster than LSTM
and performs equally well
on many tasks.

That is why GRU is widely used
in production systems.

In the next video,
we will move to Autoencoders
and learn how neural networks
can detect anomalies.

Thank you for watching.
See you in the next video.
