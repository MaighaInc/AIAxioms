Attention Mechanism from Scratch
================================

This project demonstrates how the attention
mechanism works internally using NumPy.

Requirements:
------------
- Python 3.x
- numpy

Install numpy:
--------------
pip install numpy

How to Run:
-----------
1. Open terminal
2. Navigate to this folder
3. Run:

   python attention_from_scratch.py

Expected Output:
----------------
- Attention weights
- Weighted output vector

Learning Objective:
-------------------
Understand how attention focuses on
important parts of input data.
