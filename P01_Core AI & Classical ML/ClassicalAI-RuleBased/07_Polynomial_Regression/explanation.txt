Hello guys, welcome back to the channel.

In the previous video,
we learned logistic regression,
which is used for classification.

Today we are coming back to regression,
but with a twist.

Todayâ€™s topic is:
POLYNOMIAL REGRESSION.

Now the first important thing
I want you to understand is this:

Polynomial regression is still
linear regression.

Yes, you heard that correctly.

It is called polynomial regression
because the shape of the line is curved,
but the model is still linear
with respect to its parameters.

Now let us understand the problem.

Linear regression can only draw straight lines.
But many real-world relationships
are not straight.

For example:
Studying too little gives low marks.
Studying a moderate amount gives high marks.
Studying too much causes burnout,
and marks drop again.

This relationship is curved.

Linear regression cannot model this properly.
Polynomial regression can.

Now how does polynomial regression work?

It does not change the learning algorithm.
It changes the input features.

Instead of using only x,
we also use x squared.

So the equation becomes:
y equals w1 times x
plus w2 times x squared
plus bias.

Now let us look at the code.

We created a class called PolynomialRegression.

Inside the constructor,
we define learning rate and epochs,
just like linear regression.

We now have two weights:
w1 for x
and w2 for x squared.

The predict function calculates
the polynomial equation.

The train function is very similar
to linear regression.

The only difference is that
we calculate gradients for both x and x squared.

During training,
the model adjusts w1, w2, and bias
to reduce error.

You will notice that
the loss keeps decreasing.

This means the curve is fitting better
and better.

After training,
we print the learned coefficients.

These coefficients define the shape of the curve.

Then we test the model
with a new input value.

Now let us talk about overfitting.

Polynomial regression can easily overfit
if we use very high-degree polynomials.

That is why choosing the right degree
is very important.

Polynomial regression teaches us
a very powerful lesson:

Complex behavior does not always need
complex algorithms.

Sometimes,
simple models with better features
are enough.

This concept is extremely important
for understanding neural networks later.

In the next video,
we will learn about the Perceptron,
which is the first neural network ever created.

Thank you for watching.
See you in the next video.
