Hello everyone, welcome back to the channel.

In the previous video,
we learned about decision trees,
which make decisions using learned rules.

Today we are learning something completely different.

Today’s topic is:
NAIVE BAYES TEXT CLASSIFIER.

This algorithm does not use rules.
It does not use distance.
It uses probability.

Naive Bayes is one of the most widely used
algorithms in real-world applications,
especially spam email filtering.

Let us understand the problem first.

We want to classify text messages into two categories:
Spam and Ham.

Spam means unwanted messages.
Ham means normal messages.

Now the biggest challenge with text is:
Computers do not understand words.

So how does Naive Bayes solve this?

It uses a very simple idea.

It asks the question:
“How likely is this message to belong to Spam,
and how likely is it to belong to Ham?”

Whichever probability is higher,
that class is chosen.

Now let us look at the code.

We created a class called NaiveBayesClassifier.

Inside the constructor,
we initialize three things.

First,
a dictionary to store word counts per class.

Second,
a dictionary to store how many messages belong to each class.

Third,
a vocabulary set to store all unique words.

Now let us move to the train function.

The train function takes labeled data.
Each data point has a text and a label.

For every message,
we increase the count of its class.

Then we split the text into words.
For each word,
we count how many times it appears in that class.

This is how the model learns.

Now comes the most important part:
the predict function.

For a new message,
we calculate a score for each class.

We start with the prior probability,
which means how common that class is.

Then for every word in the message,
we calculate the probability of that word
given the class.

We use something called Laplace smoothing
to avoid zero probability.

We add log probabilities
to avoid numerical underflow.

Finally,
the class with the highest score
is returned as the prediction.

Now let us test the model.

The message is:
Win free money now.

Words like win, free, and money
appear mostly in spam messages.

So the probability of Spam
becomes higher than Ham.

Hence the prediction is Spam.

Now let us talk about why this algorithm is called naive.

It assumes that all words are independent.
In real life, this is not true.

But surprisingly,
this assumption works very well in practice.

Naive Bayes is fast,
simple,
and works well even with small datasets.

That is why it is still used today
in email systems and document classification.

In the next video,
we will learn about K-Nearest Neighbors,
which uses distance instead of probability.

If you understand Naive Bayes clearly,
you now understand probabilistic machine learning.

Thank you for watching.
See you in the next video.
