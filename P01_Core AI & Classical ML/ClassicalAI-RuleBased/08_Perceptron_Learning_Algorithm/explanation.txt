Hello guys, welcome back to the channel.

So far, we have learned regression models
and probabilistic models.

Today we are stepping into the world
of neural networks.

Todayâ€™s topic is:
THE PERCEPTRON LEARNING ALGORITHM.

The perceptron is the first neural network
ever created.

It was introduced in 1957
and laid the foundation
for modern deep learning.

Now let us understand the idea intuitively.

The perceptron is inspired
by how a biological neuron works.

A neuron receives inputs,
multiplies them by weights,
adds them together,
and then decides whether to fire or not.

That is exactly what we are implementing here.

Now let us look at the code.

We created a class called Perceptron.

The constructor initializes
learning rate,
number of epochs,
weights,
and bias.

The activation function
is a step function.

If the weighted sum is positive,
the output is 1.
Otherwise, it is 0.

The predict function calculates
the weighted sum
and applies the activation function.

Now the most important part
is the train function.

The perceptron learns
by correcting its mistakes.

For every training example,
the perceptron makes a prediction.

If the prediction is wrong,
it updates the weights and bias.

The update rule is:
new weight equals old weight
plus learning rate
times error
times input.

This update pushes the decision boundary
toward the correct direction.

You will notice that
the number of misclassifications
reduces over epochs.

Once there are no mistakes,
training stops.

This is called convergence.

Now let us talk about limitations.

The perceptron can only learn
linearly separable data.

It cannot solve problems like XOR.

This limitation led to
the development of multi-layer networks
and backpropagation.

However,
understanding the perceptron
is critical for understanding deep learning.

Every modern neural network
is an extension of this idea.

In the next video,
we will move to Genetic Algorithms,
which use evolution instead of gradients.

Thank you for watching.
See you in the next video.
