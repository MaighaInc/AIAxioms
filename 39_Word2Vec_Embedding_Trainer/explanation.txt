Hello guys, welcome back to the channel.

In the previous videos,
we represented words using counts.

Bag-of-Words and TF-IDF
count words,
but they do not understand meaning.

Today that changes.

Todayâ€™s topic is:
WORD2VEC EMBEDDING TRAINER.

Word2Vec was a breakthrough in NLP.

It allowed machines
to understand semantic meaning
of words.

Now let us understand the idea.

Word2Vec is based on a simple principle:

Words that appear
in similar contexts
have similar meanings.

For example:
king and queen,
apple and banana,
car and bike.

They appear in similar contexts.

Word2Vec learns this automatically.

Now let us understand the model.

We use the Skip-Gram architecture.

In Skip-Gram,
the model takes a center word
and tries to predict
the surrounding context words.

By doing this repeatedly,
the model learns word embeddings.

Now let us look at the code.

We first create a small corpus.

Then we tokenize the text
and build a vocabulary.

Each word is assigned an index.

Next,
we generate skip-gram pairs.

For each word,
we pair it with neighboring words
within a window.

These pairs are training examples.

Now let us look at the model.

We initialize two weight matrices.

One for input embeddings,
one for output embeddings.

During training,
we take a center word,
predict context probabilities,
and compute error.

We update weights
using gradient descent.

After training,
each word has a vector.

These vectors capture meaning.

Words used in similar contexts
end up close in vector space.

This is why word embeddings are powerful.

Word2Vec embeddings are used in:
search engines,
recommendation systems,
text similarity,
and many NLP pipelines.

In the next video,
we will learn GloVe,
which learns embeddings
using global statistics.

Thank you for watching.
See you in the next video.
