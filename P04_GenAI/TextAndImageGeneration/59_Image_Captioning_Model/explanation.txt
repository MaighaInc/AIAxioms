Hello guys, welcome back to the channel.

In the previous video,
we learned how diffusion models
generate images from noise.

Today we do the opposite.

We take an image
and generate text from it.

Todayâ€™s topic is:
IMAGE CAPTIONING MODEL.

Image captioning is a classic
multi-modal AI problem.

Multi-modal means:
the model works with
more than one type of data.

In this case:
images and text.

Now let us understand the architecture.

Image captioning systems
have two main components.

First:
a vision model.

This is usually a CNN.

The CNN looks at the image
and extracts visual features.

These features represent
objects and patterns.

Second:
a language model.

This model takes the visual features
and generates a sentence.

Now let us look at the simplified code.

We simulate the CNN
by randomly selecting
a detected object.

In real systems,
this comes from deep visual features.

Then we use templates
to generate a caption.

This simulates the language model.

Even though this is simplified,
the flow is exactly the same
as real image captioning systems.

Image captioning is used in:
assistive technology,
search engines,
image indexing,
and accessibility tools.

Modern systems use
transformers for both vision and language.

In the next video,
we will apply image generation ideas
to style transfer,
where AI transforms images artistically.

Thank you for watching.
See you in the next video.
