Hello guys, welcome back to the channel.

Today we officially begin
PART 3 of our AI journey:
NATURAL LANGUAGE PROCESSING.

Before machines can understand language,
we must first break text into pieces.

Todayâ€™s topic is:
TOKENIZER FROM SCRATCH.

Tokenization is the foundation
of all NLP systems.

Search engines,
chatbots,
translation systems,
and language models
all begin with tokenization.

Now let us understand the problem.

Computers do not understand text.

They only understand numbers.

So the first step in NLP
is converting raw text
into smaller units called tokens.

Tokens can be:
characters,
words,
or sentences.

Now let us look at the code.

We created a Tokenizer class
from scratch.

First,
we have character-level tokenization.

This simply splits text
into individual characters.

This is used in
character-level language models.

Next,
we have word-level tokenization.

Before splitting words,
we do two important steps.

First,
we convert text to lowercase.

This avoids treating
Hello and hello
as different words.

Second,
we remove punctuation.

Punctuation usually does not add
meaning for most NLP tasks.

Then we split the text by spaces.

This gives us word tokens.

Next,
we have sentence-level tokenization.

We split text
based on punctuation like
periods,
question marks,
and exclamation marks.

Now let us see why this matters.

Every NLP task depends on tokens.

Bag-of-Words uses word tokens.
TF-IDF uses word tokens.
Word2Vec uses word tokens.
Transformers use subword tokens.

If tokenization is wrong,
everything that follows is wrong.

That is why understanding tokenization
is extremely important.

In the next video,
we will build a Bag-of-Words classifier
using these tokens.

This will be our first
machine learning NLP model.

Thank you for watching.
See you in the next video.
