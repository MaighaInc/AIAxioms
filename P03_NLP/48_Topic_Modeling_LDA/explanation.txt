Hello guys, welcome back to the channel.

In the previous video,
we learned document clustering.

Clustering groups documents.

Today we go one step deeper.

Todayâ€™s topic is:
TOPIC MODELING USING LDA.

Topic modeling answers a different question.

Instead of grouping documents,
we ask:
what topics exist inside the documents?

And how much of each topic
is present in each document?

This is extremely powerful.

Now let us understand the idea.

A topic is a group of words
that often appear together.

For example:
machine, learning, neural
might form one topic.

football, team, match
might form another topic.

LDA assumes:
each document is a mixture of topics,
and each topic is a mixture of words.

Now let us look at the code.

We use a simplified version of LDA.

First,
we tokenize documents.

Then we randomly assign
a topic to each word.

This may sound strange,
but this randomness
is the starting point.

Next,
we use an iterative process
called Gibbs sampling.

For each word,
we temporarily remove its topic
and calculate probabilities
for all topics.

These probabilities depend on:
how often the word appears in a topic,
and how dominant the topic is in the document.

We then assign the word
to the most likely topic.

Over many iterations,
topics stabilize.

Finally,
we extract the top words
from each topic.

These words describe the topic.

Topic modeling is used in:
news analysis,
document organization,
trend discovery,
and recommendation systems.

In the next video,
we will build a resume parser,
which uses multiple NLP techniques together.

Thank you for watching.
See you in the next video.
