Algorithm: Naive Bayes

Goal:
Classification using Bayes' Theorem.

Core Assumption:
Features are conditionally independent.

Formula:
P(y|X) ‚àù P(X|y) P(y)

Variants:
- Gaussian
- Multinomial
- Bernoulli

Strengths:
- Fast
- Works well for text data

Weaknesses:
- Independence assumption unrealistic

Type:
Supervised, Probabilistic

*******************************************************
Algorithm: Gaussian Mixture Model (GMM)

Goal:
Soft clustering using probability distributions.

How it works:
- Assume data is generated by multiple Gaussians
- Use Expectation-Maximization (EM)

Output:
Probability of belonging to each cluster

Strengths:
- Soft assignments
- Flexible cluster shapes

Weaknesses:
- Requires number of components
- Can converge to local minima

Type:
Unsupervised, Probabilistic

***********************************************************
